---
title: "Lab3 Notebook"
output:
  pdf_document:
    toc: true
    number_sections: true
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
library(ggplot2)
library(fit.models)
library(tidyr)
library('reshape2')
library(robust)
library("MASS")
library("jpeg")
library(tclust, quietly = TRUE)
library(EMCluster, quietly = TRUE)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
```{r}
# Exercice1 ----------------------------------------------------------------------
set.seed(123)
datas <- rt(15, 1)
mean(datas) #23.03012
#The mean is not a good summary because it should be near to 0. 
#It is due to outliers points and in particular a 315

summary(datas)
df = data.frame(datas)
ggplot(df, aes(x=datas)) +
  geom_histogram(binwidth=.5, colour="black", fill="white")

#We can propose the median which is less influenced to outliers
median(datas)
#The trimmed mean
mean(datas, trim = 0.2)
#Or the standart Deviation
sd(datas)
```
```{r}
# Exercice2 ----------------------------------------------------------------------
toyData = data.frame('x' = c(1,2,3,2,0,3,4,1,2,3,0,1,2,2,3,0,3,3,4,3,3,4,5))
ggplot(toyData, aes(x = '', y=x)) + 
  geom_boxplot()
```
Here we have a dataset with values between 0 and 4 and one data equal to 5. 5 is an outlier but it is not shown by the boxplot.
```{r}
# Exercice3 ----------------------------------------------------------------------

#Briefly describe the dataset.p

#The dataset contains datas about 87 characters of Star Wars.
#It is physical datas like name, height or skin color 
#but we have the list of movies were they appear, their vehicls and starships too 

dfStarWars <- dplyr::starwars

#Consider the variables height and mass, plot univariate charts to study the range and other summary statistics

summary(dfStarWars['height'])
summary(dfStarWars['mass'])
```
```{r}
ggplot(dfStarWars, aes(x = height)) + geom_density() + ggtitle("Height density")
ggplot(dfStarWars, aes(x = mass)) + geom_density() + ggtitle("Mass density") + scale_x_log10()
```
```{r}
#Plot mass vs height and describe it
ggplot(dfStarWars, aes(x = height, y = mass)) + geom_point() + ggtitle("Height vs Mass")

#The mass of a character seems connected to its height. For characters between 150 and 250 cm we could do 
#a linear regression because it seems to be a straight.
#We have an extreme outlier (1000kg and 175cm) which correspond to Jabba

#Fit two different regression models. Plot and discuss the results. Useful: lmrob or MASS::rlm
sub <- subset(dfStarWars, select=c(mass,height,name))
sub <- drop_na(sub)
lmRegression <- lm(sub)
rlmRegression <- rlm(x = as.numeric(unlist(sub['mass'])) ,y = as.numeric(unlist(sub['height'])))
```
```{r}
print(lmRegression)
summary(rlmRegression)
```

```{r}
heightAndMassGraph <- ggplot(dfStarWars, aes(x = height, y = mass))
heightAndMassGraph <- heightAndMassGraph + geom_point(position = "jitter") 
heightAndMassGraph <- heightAndMassGraph + ggtitle("Height vs Mass")+geom_smooth(method = rlm, color = 'blue')
heightAndMassGraph + geom_smooth(method = lm, color = 'red', fill = 'green')
```
We see that the robust regression in blue is further the outlier point. So this model is less affected by as expected outliers
The confidence interval in green is for the lm model.
```{r}
weights <- data.frame(rlmRegression['w'])
print(weights)
```
We have a lot points which have a weight of 1 M-estimators of 1 some. 6 points have a weight between 0.7 and 1 and we have one point whith a weight of 0.03. We will plot them with different colors.
```{r}
sub['weight'] = weights
WeightMGraph  <- ggplot(sub, aes(x = height, y = mass, color = weight))
WeightMGraph + geom_point()
```
We can see that it is the outlier point which have the lowest weight value.
```{r}
print(sub[which.min(sub$weight),])
```
The outlier point is Jabba Desilijic Tiure as expected
```{r}
JabbaImage <- readJPEG("personnage-jabba-the-hutt.jpg",native=TRUE)
plot(0:1,0:1,type="n",ann=FALSE,axes=FALSE)
rasterImage(JabbaImage,0,0,1,1)
```
```{r}
sub <- sub[-which.min(sub$weight),] # On supprime la ligne de Jabba
```
We redo the same thing without Jabba
```{r}
sub <- subset(sub, select=c(mass,height,name))
lmRegression <- lm(sub)
rlmRegression <- rlm(x = as.numeric(unlist(sub['mass'])) ,y = as.numeric(unlist(sub['height'])))
heightAndMassGraph <- ggplot(sub, aes(x = height, y = mass))
heightAndMassGraph <- heightAndMassGraph + geom_point(position = "jitter") 
heightAndMassGraph <- heightAndMassGraph + ggtitle("Height vs Mass")+geom_smooth(method = rlm, color = 'blue')
heightAndMassGraph + geom_smooth(method = lm, color = 'red', fill = 'green')
```
```{r}
weights <- data.frame(rlmRegression['w'])
print(weights)
```
```{r}
sub['weight'] = weights
WeightMGraph  <- ggplot(sub, aes(x = height, y = mass, color = weight))
WeightMGraph + geom_point()
```
```{r}
print(sub[which.min(sub$weight),])
```
The new character with the lowest weight is the general Grievous
```{r}
newdata=data.frame("name"="Luke Skywalker", "mass", "height" = 170)
result=predict(lmRegression,newdata)
print(result)
```
According to our model a 1m70 character will do 72.3 kg

```{r}
# Exercice4 ----------------------------------------------------------------------
x1 = mvrnorm(n = 20, 2, 0.1, tol = 1e-06, empirical = FALSE)
y1 = mvrnorm(n = 20, 2, 0.1, tol = 1e-06, empirical = FALSE)
x2 = mvrnorm(n = 20, 2, 0.1, tol = 1e-06, empirical = FALSE)
y2 = mvrnorm(n = 20, -4, 0.1, tol = 1e-06, empirical = FALSE)
x3 = mvrnorm(n = 20, -1, 0.1, tol = 1e-06, empirical = FALSE)
y3 = mvrnorm(n = 20, 3, 0.1, tol = 1e-06, empirical = FALSE)
xUniform = round(runif(25, min=-5, max=5), digits=2)
yUniform = round(runif(25, min=-5, max=5), digits=2)
label = c(rep("1",20),rep("2",20),rep("3",20),rep("4",25))

df = data.frame("X" = c(x1,x2,x3,xUniform), "Y" = c(y1,y2,y3,yUniform),"label" = label)
ggplot(df, aes(x = X, y = Y,color = label)) + geom_point() + ggtitle("Datas with their labels") 
```
We create a data set with 3 rectangles where 15 points are normally distributed and 20 other points uniformely distributed.
```{r}
clusters <- hclust(dist(df[, 1:2]))
print(clusters)
clusterCut <- cutree(clusters, 4)
print(clusterCut)
clusterCut<- as.character(clusterCut)
dfCluster = data.frame("X" = c(x1,x2,x3,xUniform), "Y" = c(y1,y2,y3,yUniform),"label2" = clusterCut)
ggplot(dfCluster, aes(x = X, y = Y,color = label2)) + geom_point() + ggtitle("Datas with their cluster for the tclust method")
```

```{r}
emobj <- simple.init(df[, 1:2], nclass = 4)
emobj <- shortemcluster(df[, 1:2], emobj)
summary(emobj)

ret <- emcluster(df[, 1:2], emobj, assign.class = TRUE)
clusterPredict <- ret$class
clusterPredict<- as.character(clusterPredict)
dfCluster = data.frame("X" = c(x1,x2,x3,xUniform), "Y" = c(y1,y2,y3,yUniform),"label3" = clusterPredict)
ggplot(dfCluster, aes(x = X, y = Y,color = label3)) + geom_point() + ggtitle("Datas with their cluster for the tclust method")
```
The second algorithm has bad results. He is not able to distinguish two clusters which are cleary separated. Whereas the robust algorithm has issues only on the points generates uniformally.